# Batch ì‹œìŠ¤í…œ í˜„í™© ë¶„ì„ ë° ë°ì´í„° ìˆ˜ì§‘ ê°€ì´ë“œ

## ëª©ì°¨
1. [Spring Batch ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬](#1-spring-batch-ë©”íƒ€ë°ì´í„°-ì¿¼ë¦¬)
2. [ì„±ëŠ¥ ì¸¡ì • ë„êµ¬ ì¶”ê°€](#2-ì„±ëŠ¥-ì¸¡ì •-ë„êµ¬-ì¶”ê°€)
3. [ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸](#3-ë¡œê·¸-ë¶„ì„-ìŠ¤í¬ë¦½íŠ¸)
4. [ë°ì´í„° ìˆ˜ì§‘ ì ˆì°¨](#4-ë°ì´í„°-ìˆ˜ì§‘-ì ˆì°¨)
5. [ê¸°ëŒ€ ë©”íŠ¸ë¦­ ë° KPI](#5-ê¸°ëŒ€-ë©”íŠ¸ë¦­-ë°-kpi)

---

## 1. Spring Batch ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬

Spring BatchëŠ” ì‹¤í–‰ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤. ë‹¤ìŒ ì¿¼ë¦¬ë¡œ í˜„ì¬ ìƒíƒœë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1.1 ìµœê·¼ ë°°ì¹˜ ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ

```sql
-- ìµœê·¼ 30ì¼ê°„ ë°°ì¹˜ Job ì‹¤í–‰ í†µê³„
SELECT
    ji.job_name,
    COUNT(*) as total_executions,
    COUNT(CASE WHEN je.status = 'COMPLETED' THEN 1 END) as successful,
    COUNT(CASE WHEN je.status = 'FAILED' THEN 1 END) as failed,
    ROUND(AVG(EXTRACT(EPOCH FROM (je.end_time - je.start_time))), 2) as avg_duration_seconds,
    MAX(EXTRACT(EPOCH FROM (je.end_time - je.start_time))) as max_duration_seconds,
    MIN(EXTRACT(EPOCH FROM (je.end_time - je.start_time))) as min_duration_seconds
FROM batch_job_execution je
JOIN batch_job_instance ji ON je.job_instance_id = ji.job_instance_id
WHERE je.create_time >= NOW() - INTERVAL '30 days'
GROUP BY ji.job_name
ORDER BY ji.job_name;
```

**ì˜ˆìƒ ì¶œë ¥:**
```
job_name              | total | success | failed | avg_sec | max_sec | min_sec
---------------------|-------|---------|--------|---------|---------|--------
crawlPostJob         |    30 |      25 |      5 |  450.23 | 1200.50 |  180.30
summarizePostJob     |    28 |      20 |      8 |  3600.5 | 7200.00 | 1800.00
summaryAndEmbedding  |    15 |      10 |      5 |  5400.2 | 9000.00 | 3000.00
```

### 1.2 Step ë³„ ì„±ëŠ¥ ë¶„ì„

```sql
-- Stepë³„ ì²˜ë¦¬ëŸ‰ ë° Skip/Failure í†µê³„
SELECT
    ji.job_name,
    se.step_name,
    COUNT(*) as executions,
    ROUND(AVG(se.read_count), 2) as avg_read,
    ROUND(AVG(se.write_count), 2) as avg_write,
    SUM(se.skip_count) as total_skipped,
    SUM(se.rollback_count) as total_rollbacks,
    ROUND(AVG(EXTRACT(EPOCH FROM (se.end_time - se.start_time))), 2) as avg_step_duration_sec
FROM batch_step_execution se
JOIN batch_job_execution je ON se.job_execution_id = je.job_execution_id
JOIN batch_job_instance ji ON je.job_instance_id = ji.job_instance_id
WHERE se.start_time >= NOW() - INTERVAL '30 days'
GROUP BY ji.job_name, se.step_name
ORDER BY ji.job_name, se.step_name;
```

**ì˜ˆìƒ ì¶œë ¥:**
```
job_name         | step_name              | avg_read | avg_write | total_skip | avg_sec
----------------|------------------------|----------|-----------|------------|--------
crawlPostJob    | crawlPostStep          |    13.00 |      8.50 |         45 |  420.00
summarizePost   | summarizePostStep      |   120.50 |    110.20 |        105 | 3200.00
```

### 1.3 ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„

```sql
-- ìµœê·¼ ì‹¤íŒ¨í•œ Jobì˜ ìƒì„¸ ì •ë³´
SELECT
    ji.job_name,
    je.job_execution_id,
    je.start_time,
    je.end_time,
    EXTRACT(EPOCH FROM (je.end_time - je.start_time)) as duration_seconds,
    je.status,
    je.exit_code,
    je.exit_message,
    se.step_name,
    se.read_count,
    se.write_count,
    se.skip_count,
    se.rollback_count
FROM batch_job_execution je
JOIN batch_job_instance ji ON je.job_instance_id = ji.job_instance_id
LEFT JOIN batch_step_execution se ON je.job_execution_id = se.job_execution_id
WHERE je.status != 'COMPLETED'
  AND je.start_time >= NOW() - INTERVAL '7 days'
ORDER BY je.start_time DESC
LIMIT 50;
```

### 1.4 ì²˜ë¦¬ëŸ‰ ì¶”ì´ ë¶„ì„

```sql
-- ì¼ë³„ ì²˜ë¦¬ëŸ‰ ì¶”ì´ (ìµœê·¼ 30ì¼)
SELECT
    DATE(je.start_time) as execution_date,
    ji.job_name,
    COUNT(*) as executions,
    SUM(se.write_count) as total_items_processed,
    ROUND(AVG(EXTRACT(EPOCH FROM (je.end_time - je.start_time))), 2) as avg_duration_sec
FROM batch_job_execution je
JOIN batch_job_instance ji ON je.job_instance_id = ji.job_instance_id
LEFT JOIN batch_step_execution se ON je.job_execution_id = se.job_execution_id
WHERE je.start_time >= NOW() - INTERVAL '30 days'
  AND je.status = 'COMPLETED'
GROUP BY DATE(je.start_time), ji.job_name
ORDER BY execution_date DESC, ji.job_name;
```

---

## 2. ì„±ëŠ¥ ì¸¡ì • ë„êµ¬ ì¶”ê°€

### 2.1 Enhanced Logging Listener (ì„ì‹œ ì„±ëŠ¥ ì¸¡ì •ìš©)

**íŒŒì¼ ìœ„ì¹˜:** `batch/src/main/kotlin/com/techinsights/batch/listener/BaselineMetricsListener.kt`

```kotlin
package com.techinsights.batch.listener

import org.slf4j.LoggerFactory
import org.springframework.batch.core.JobExecution
import org.springframework.batch.core.JobExecutionListener
import org.springframework.batch.core.StepExecution
import org.springframework.batch.core.StepExecutionListener
import org.springframework.stereotype.Component
import java.time.Duration
import java.time.LocalDateTime

/**
 * ê°œì„  ì „ Baseline ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ ì„ì‹œ Listener
 *
 * ìˆ˜ì§‘ ë°ì´í„°:
 * - ì „ì²´ Job ì†Œìš” ì‹œê°„
 * - Stepë³„ ì†Œìš” ì‹œê°„
 * - ì²˜ë¦¬ëŸ‰ (items/sec)
 * - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
 */
@Component
class BaselineMetricsListener : JobExecutionListener, StepExecutionListener {

    private val log = LoggerFactory.getLogger(BaselineMetricsListener::class.java)

    private data class StepMetrics(
        val stepName: String,
        val startTime: LocalDateTime,
        var endTime: LocalDateTime? = null,
        var readCount: Long = 0,
        var writeCount: Long = 0,
        var skipCount: Long = 0
    )

    private val stepMetricsMap = mutableMapOf<Long, StepMetrics>()

    override fun beforeJob(jobExecution: JobExecution) {
        log.info("""
            ========================================
            ğŸ“Š BASELINE METRICS - Job Starting
            ========================================
            Job Name: ${jobExecution.jobInstance.jobName}
            Job ID: ${jobExecution.jobExecutionId}
            Start Time: ${jobExecution.startTime}
            Parameters: ${jobExecution.jobParameters}

            System Info:
            - Available Processors: ${Runtime.getRuntime().availableProcessors()}
            - Max Memory: ${Runtime.getRuntime().maxMemory() / 1024 / 1024} MB
            - Free Memory: ${Runtime.getRuntime().freeMemory() / 1024 / 1024} MB
            ========================================
        """.trimIndent())
    }

    override fun afterJob(jobExecution: JobExecution) {
        val duration = Duration.between(jobExecution.startTime, jobExecution.endTime)
        val totalSeconds = duration.seconds

        val totalRead = jobExecution.stepExecutions.sumOf { it.readCount }
        val totalWrite = jobExecution.stepExecutions.sumOf { it.writeCount }
        val totalSkip = jobExecution.stepExecutions.sumOf { it.skipCount }

        val throughput = if (totalSeconds > 0) totalWrite.toDouble() / totalSeconds else 0.0

        log.info("""
            ========================================
            ğŸ“Š BASELINE METRICS - Job Completed
            ========================================
            Job Name: ${jobExecution.jobInstance.jobName}
            Status: ${jobExecution.status}
            Exit Code: ${jobExecution.exitStatus.exitCode}

            â±ï¸  Duration:
            - Total: ${formatDuration(duration)}
            - Start: ${jobExecution.startTime}
            - End: ${jobExecution.endTime}

            ğŸ“ˆ Processing Stats:
            - Items Read: $totalRead
            - Items Written: $totalWrite
            - Items Skipped: $totalSkip
            - Throughput: ${String.format("%.2f", throughput)} items/sec

            ğŸ’¾ Memory Usage:
            - Max Memory: ${Runtime.getRuntime().maxMemory() / 1024 / 1024} MB
            - Total Memory: ${Runtime.getRuntime().totalMemory() / 1024 / 1024} MB
            - Free Memory: ${Runtime.getRuntime().freeMemory() / 1024 / 1024} MB
            - Used Memory: ${(Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory()) / 1024 / 1024} MB

            ğŸ”§ Step Breakdown:
            ${generateStepBreakdown(jobExecution)}

            ${if (jobExecution.allFailureExceptions.isNotEmpty()) {
                """
                âŒ Failures:
                ${jobExecution.allFailureExceptions.joinToString("\n") {
                    "- ${it.javaClass.simpleName}: ${it.message}"
                }}
                """.trimIndent()
            } else ""}
            ========================================
        """.trimIndent())

        // CSV í˜•ì‹ìœ¼ë¡œë„ ì¶œë ¥ (ë¶„ì„ ìš©ì´)
        log.info("BASELINE_CSV,${jobExecution.jobInstance.jobName},${jobExecution.jobExecutionId}," +
                "${totalSeconds},${totalRead},${totalWrite},${totalSkip},${throughput}," +
                "${jobExecution.status},${jobExecution.exitStatus.exitCode}")
    }

    override fun beforeStep(stepExecution: StepExecution): Unit {
        val metrics = StepMetrics(
            stepName = stepExecution.stepName,
            startTime = stepExecution.startTime
        )
        stepMetricsMap[stepExecution.id] = metrics

        log.info("ğŸ”¹ Step [${stepExecution.stepName}] starting at ${stepExecution.startTime}")
    }

    override fun afterStep(stepExecution: StepExecution): org.springframework.batch.core.ExitStatus {
        val metrics = stepMetricsMap[stepExecution.id]
        metrics?.endTime = stepExecution.endTime
        metrics?.readCount = stepExecution.readCount
        metrics?.writeCount = stepExecution.writeCount
        metrics?.skipCount = stepExecution.skipCount

        val duration = Duration.between(stepExecution.startTime, stepExecution.endTime)
        val seconds = duration.seconds
        val throughput = if (seconds > 0) stepExecution.writeCount.toDouble() / seconds else 0.0

        log.info("""
            ğŸ”¹ Step [${stepExecution.stepName}] completed
            - Duration: ${formatDuration(duration)}
            - Read: ${stepExecution.readCount}
            - Write: ${stepExecution.writeCount}
            - Skip: ${stepExecution.skipCount}
            - Rollback: ${stepExecution.rollbackCount}
            - Throughput: ${String.format("%.2f", throughput)} items/sec
        """.trimIndent())

        return stepExecution.exitStatus
    }

    private fun generateStepBreakdown(jobExecution: JobExecution): String {
        return jobExecution.stepExecutions.joinToString("\n") { step ->
            val duration = Duration.between(step.startTime, step.endTime)
            val seconds = duration.seconds
            val throughput = if (seconds > 0) step.writeCount.toDouble() / seconds else 0.0

            """
            â”‚ ${step.stepName}
            â”‚   Duration: ${formatDuration(duration)}
            â”‚   Read: ${step.readCount} | Write: ${step.writeCount} | Skip: ${step.skipCount}
            â”‚   Throughput: ${String.format("%.2f", throughput)} items/sec
            """.trimIndent()
        }
    }

    private fun formatDuration(duration: Duration): String {
        val hours = duration.toHours()
        val minutes = duration.toMinutes() % 60
        val seconds = duration.seconds % 60

        return when {
            hours > 0 -> "${hours}h ${minutes}m ${seconds}s"
            minutes > 0 -> "${minutes}m ${seconds}s"
            else -> "${seconds}s"
        }
    }
}
```

### 2.2 Companyë³„ ì²˜ë¦¬ ì‹œê°„ ì¶”ì  Processor

**íŒŒì¼ ìœ„ì¹˜:** `batch/src/main/kotlin/com/techinsights/batch/processor/MetricsTrackingRawPostProcessor.kt`

```kotlin
package com.techinsights.batch.processor

import com.techinsights.batch.crawling.PostCrawlingService
import com.techinsights.domain.dto.company.CompanyDto
import com.techinsights.domain.dto.post.PostDto
import kotlinx.coroutines.runBlocking
import org.springframework.batch.item.ItemProcessor
import org.springframework.stereotype.Component
import java.util.concurrent.ConcurrentHashMap
import java.util.concurrent.atomic.AtomicInteger

/**
 * Baseline ì¸¡ì •ìš© - ê¸°ì¡´ RawPostProcessorë¥¼ í™•ì¥í•˜ì—¬ íšŒì‚¬ë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
 */
@Component("metricsTrackingRawPostProcessor")
class MetricsTrackingRawPostProcessor(
    private val postCrawlingService: PostCrawlingService
) : ItemProcessor<CompanyDto, List<PostDto>> {

    companion object {
        private val log = org.slf4j.LoggerFactory.getLogger(MetricsTrackingRawPostProcessor::class.java)

        // ì „ì—­ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Job ì‹¤í–‰ ê°„ ê³µìœ )
        private val companyProcessingTimes = ConcurrentHashMap<String, MutableList<Long>>()
        private val currentIndex = AtomicInteger(0)
        private var totalCompanies = 0
    }

    override fun process(company: CompanyDto): List<PostDto> {
        if (totalCompanies == 0) {
            // ì²« ì‹¤í–‰ ì‹œ ì´ˆê¸°í™” (ì‹¤ì œë¡œëŠ” CompanyReaderì—ì„œ total countë¥¼ ê°€ì ¸ì™€ì•¼ í•¨)
            totalCompanies = 13  // í˜„ì¬ íšŒì‚¬ ìˆ˜
            currentIndex.set(0)
        }

        val current = currentIndex.incrementAndGet()
        val progress = (current.toDouble() / totalCompanies * 100).toInt()

        log.info("ğŸ”„ [$current/$totalCompanies] ($progress%) Processing: ${company.name}")

        val startTime = System.currentTimeMillis()

        return runBlocking {
            try {
                val result = postCrawlingService.processCrawledData(company)
                val duration = System.currentTimeMillis() - startTime

                // ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
                companyProcessingTimes
                    .computeIfAbsent(company.name) { mutableListOf() }
                    .add(duration)

                log.info("âœ… [$current/$totalCompanies] ${company.name}: " +
                        "${result.size} posts in ${duration}ms (${duration/1000}s)")

                // CSV í˜•ì‹ ë¡œê·¸ (ë¶„ì„ìš©)
                log.info("COMPANY_METRIC,${company.name},${result.size},$duration")

                result
            } catch (e: Exception) {
                val duration = System.currentTimeMillis() - startTime
                log.error("âŒ [$current/$totalCompanies] ${company.name} FAILED after ${duration}ms: ${e.message}")

                log.info("COMPANY_METRIC,${company.name},0,$duration,FAILED,${e.javaClass.simpleName}")

                throw e
            }
        }
    }

    // Job ì¢…ë£Œ í›„ í˜¸ì¶œí•˜ì—¬ í†µê³„ ì¶œë ¥
    fun printStatistics() {
        log.info("""
            ========================================
            ğŸ“Š Company Processing Statistics
            ========================================
            ${companyProcessingTimes.entries.sortedByDescending {
                it.value.average()
            }.joinToString("\n") { (company, times) ->
                val avg = times.average()
                val min = times.minOrNull() ?: 0
                val max = times.maxOrNull() ?: 0
                "$company: avg=${avg.toLong()}ms, min=${min}ms, max=${max}ms, executions=${times.size}"
            }}
            ========================================
        """.trimIndent())
    }
}
```

### 2.3 Listener ì„¤ì •ì— ì¶”ê°€

**íŒŒì¼ ìœ„ì¹˜:** `batch/src/main/kotlin/com/techinsights/batch/config/PostCrawlingBatchConfig.kt`

ê¸°ì¡´ íŒŒì¼ì— ìƒˆë¡œìš´ Listener ì¶”ê°€:

```kotlin
@Configuration
class PostCrawlingBatchConfig (
  private val jobRepository: JobRepository,
  private val transactionManager: PlatformTransactionManager,
  private val companyReader: CompanyReader,
  private val rawPostProcessor: RawPostProcessor,
  private val rawPostWriter: RawPostWriter,
  private val properties: PostCrawlingBatchProperties,
  private val loggingJobExecutionListener: LoggingJobExecutionListener,
  private val baselineMetricsListener: BaselineMetricsListener  // ì¶”ê°€
){

  @Bean
  fun crawlPostJob(@Qualifier("crawlPostStep") crawlPostStep: Step): Job =
    JobBuilder(properties.jobName, jobRepository)
      .incrementer(RunIdIncrementer())
      .listener(loggingJobExecutionListener)
      .listener(baselineMetricsListener)  // ì¶”ê°€
      .start(crawlPostStep)
      .build()

  // ... ë‚˜ë¨¸ì§€ ì½”ë“œ
}
```

---

## 3. ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

### 3.1 ë¡œê·¸ íŒŒì‹± ìŠ¤í¬ë¦½íŠ¸ (Python)

**íŒŒì¼ ìœ„ì¹˜:** `scripts/analyze_batch_logs.py`

```python
#!/usr/bin/env python3
"""
Batch ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ Baseline ë©”íŠ¸ë¦­ì„ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
  python scripts/analyze_batch_logs.py /path/to/batch.log

ì¶œë ¥:
  - ë°°ì¹˜ ì‹¤í–‰ í†µê³„
  - íšŒì‚¬ë³„ ì²˜ë¦¬ ì‹œê°„
  - ì‹¤íŒ¨ ë¶„ì„
  - CSV ë¦¬í¬íŠ¸
"""

import re
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
import statistics

class BatchLogAnalyzer:
    def __init__(self, log_file):
        self.log_file = log_file
        self.job_executions = []
        self.company_metrics = defaultdict(list)
        self.failures = []

    def parse(self):
        """ë¡œê·¸ íŒŒì¼ íŒŒì‹±"""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            current_job = None

            for line in f:
                # Job ì‹œì‘ ê°ì§€
                if 'BASELINE METRICS - Job Starting' in line:
                    current_job = {'start_line': line}

                # Job ì¢…ë£Œ ê°ì§€
                elif 'BASELINE METRICS - Job Completed' in line and current_job:
                    current_job['end_line'] = line
                    self.parse_job_metrics(current_job)
                    current_job = None

                # Company ë©”íŠ¸ë¦­ íŒŒì‹±
                elif 'COMPANY_METRIC' in line:
                    self.parse_company_metric(line)

                # CSV í˜•ì‹ ë©”íŠ¸ë¦­ íŒŒì‹±
                elif 'BASELINE_CSV' in line:
                    self.parse_csv_metric(line)

                # ì‹¤íŒ¨ ë¡œê·¸ ê°ì§€
                elif 'FAILED' in line or 'ERROR' in line:
                    self.failures.append(line)

    def parse_job_metrics(self, job_data):
        """Job ì‹¤í–‰ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        # ì •ê·œì‹ìœ¼ë¡œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        # ì‹¤ì œ êµ¬í˜„ì€ ë¡œê·¸ í˜•ì‹ì— ë§ê²Œ ì¡°ì • í•„ìš”
        pass

    def parse_company_metric(self, line):
        """íšŒì‚¬ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ

        í˜•ì‹: COMPANY_METRIC,{company},{posts},{duration_ms}[,FAILED,{error}]
        """
        parts = line.split('COMPANY_METRIC,')[1].strip().split(',')
        if len(parts) >= 3:
            company = parts[0]
            posts = int(parts[1])
            duration = int(parts[2])
            status = 'SUCCESS' if len(parts) < 4 else parts[3]

            self.company_metrics[company].append({
                'posts': posts,
                'duration_ms': duration,
                'status': status
            })

    def parse_csv_metric(self, line):
        """CSV í˜•ì‹ ë©”íŠ¸ë¦­ íŒŒì‹±

        í˜•ì‹: BASELINE_CSV,{job_name},{job_id},{duration},{read},{write},{skip},{throughput},{status},{exit_code}
        """
        parts = line.split('BASELINE_CSV,')[1].strip().split(',')
        if len(parts) >= 9:
            self.job_executions.append({
                'job_name': parts[0],
                'job_id': parts[1],
                'duration_sec': int(parts[2]),
                'read': int(parts[3]),
                'write': int(parts[4]),
                'skip': int(parts[5]),
                'throughput': float(parts[6]),
                'status': parts[7],
                'exit_code': parts[8]
            })

    def generate_report(self):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        print("=" * 80)
        print("ğŸ“Š BATCH BASELINE ANALYSIS REPORT")
        print("=" * 80)
        print()

        # Job ì‹¤í–‰ í†µê³„
        if self.job_executions:
            print("ğŸ“ˆ Job Execution Summary")
            print("-" * 80)
            for job in self.job_executions:
                print(f"Job: {job['job_name']} (ID: {job['job_id']})")
                print(f"  Duration: {job['duration_sec']}s ({job['duration_sec']//60}m {job['duration_sec']%60}s)")
                print(f"  Processed: {job['write']}/{job['read']} items")
                print(f"  Skipped: {job['skip']} items")
                print(f"  Throughput: {job['throughput']:.2f} items/sec")
                print(f"  Status: {job['status']}")
                print()

        # íšŒì‚¬ë³„ í†µê³„
        if self.company_metrics:
            print("ğŸ¢ Company Processing Statistics")
            print("-" * 80)

            stats = []
            for company, metrics in self.company_metrics.items():
                durations = [m['duration_ms'] for m in metrics]
                posts = [m['posts'] for m in metrics]
                successes = sum(1 for m in metrics if m['status'] == 'SUCCESS')

                stats.append({
                    'company': company,
                    'avg_duration': statistics.mean(durations) if durations else 0,
                    'min_duration': min(durations) if durations else 0,
                    'max_duration': max(durations) if durations else 0,
                    'avg_posts': statistics.mean(posts) if posts else 0,
                    'executions': len(metrics),
                    'success_rate': (successes / len(metrics) * 100) if metrics else 0
                })

            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
            stats.sort(key=lambda x: x['avg_duration'], reverse=True)

            print(f"{'Company':<20} {'Avg Time':<12} {'Min':<10} {'Max':<10} {'Avg Posts':<10} {'Success %':<10}")
            print("-" * 80)
            for s in stats:
                print(f"{s['company']:<20} "
                      f"{s['avg_duration']/1000:>10.1f}s "
                      f"{s['min_duration']/1000:>9.1f}s "
                      f"{s['max_duration']/1000:>9.1f}s "
                      f"{s['avg_posts']:>9.1f} "
                      f"{s['success_rate']:>9.1f}%")
            print()

        # ì‹¤íŒ¨ ë¶„ì„
        if self.failures:
            print("âŒ Failure Analysis")
            print("-" * 80)
            print(f"Total failures detected: {len(self.failures)}")

            # ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜
            error_types = defaultdict(int)
            for failure in self.failures:
                if 'TimeoutException' in failure:
                    error_types['Timeout'] += 1
                elif 'ConnectException' in failure:
                    error_types['Connection'] += 1
                elif 'ParseException' in failure or 'parsing' in failure.lower():
                    error_types['Parse Error'] += 1
                else:
                    error_types['Other'] += 1

            print("\nError Types:")
            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                print(f"  {error_type}: {count}")
            print()

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python analyze_batch_logs.py <log_file>")
        sys.exit(1)

    log_file = sys.argv[1]
    if not Path(log_file).exists():
        print(f"Error: File not found: {log_file}")
        sys.exit(1)

    analyzer = BatchLogAnalyzer(log_file)
    analyzer.parse()
    analyzer.generate_report()
```

### 3.2 ë¡œê·¸ ì¶”ì¶œ ì‰˜ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼ ìœ„ì¹˜:** `scripts/extract_batch_metrics.sh`

```bash
#!/bin/bash
# Batch ë¡œê·¸ì—ì„œ ë©”íŠ¸ë¦­ë§Œ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

LOG_FILE=${1:-/var/log/batch/batch.log}
OUTPUT_DIR=${2:-./batch-metrics}

mkdir -p "$OUTPUT_DIR"

echo "Extracting metrics from $LOG_FILE to $OUTPUT_DIR"

# Job ì‹¤í–‰ í†µê³„ ì¶”ì¶œ
echo "Extracting job executions..."
grep "BASELINE_CSV" "$LOG_FILE" > "$OUTPUT_DIR/job_executions.csv"

# íšŒì‚¬ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
echo "Extracting company metrics..."
grep "COMPANY_METRIC" "$LOG_FILE" > "$OUTPUT_DIR/company_metrics.csv"

# ì—ëŸ¬ ë¡œê·¸ ì¶”ì¶œ
echo "Extracting errors..."
grep -E "ERROR|FAILED|Exception" "$LOG_FILE" > "$OUTPUT_DIR/errors.log"

# Step í†µê³„ ì¶”ì¶œ
echo "Extracting step statistics..."
grep -A 5 "Step \[.*\] completed" "$LOG_FILE" > "$OUTPUT_DIR/step_statistics.log"

echo "Done! Metrics extracted to $OUTPUT_DIR"
ls -lh "$OUTPUT_DIR"
```

---

## 4. ë°ì´í„° ìˆ˜ì§‘ ì ˆì°¨

### Phase 1: ì‚¬ì „ ì¤€ë¹„ (1ì¼)

1. **ì½”ë“œ ë°°í¬**
   ```bash
   # BaselineMetricsListener ì¶”ê°€
   cd batch/src/main/kotlin/com/techinsights/batch/listener
   # ìœ„ì˜ BaselineMetricsListener.kt íŒŒì¼ ìƒì„±

   # ë¹Œë“œ ë° ë°°í¬
   ./gradlew :batch:build
   # ì„œë²„ì— ë°°í¬
   ```

2. **ë¡œê·¸ ë ˆë²¨ ì„¤ì •**
   ```yaml
   # application.ymlì— ì¶”ê°€
   logging:
     level:
       com.techinsights.batch: INFO
       org.springframework.batch: INFO
     file:
       name: /var/log/batch/batch.log
       max-size: 100MB
       max-history: 30
   ```

### Phase 2: ë°ì´í„° ìˆ˜ì§‘ (7-14ì¼)

1. **ì¼ì¼ ë°°ì¹˜ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§**
   ```bash
   # ë°°ì¹˜ ì‹¤í–‰
   java -jar batch.jar --spring.batch.job.names=crawlPostJob

   # ë¡œê·¸ í™•ì¸
   tail -f /var/log/batch/batch.log | grep "BASELINE"
   ```

2. **ì¼ì¼ ë©”íŠ¸ë¦­ ì¶”ì¶œ**
   ```bash
   # ë§¤ì¼ ì‹¤í–‰
   ./scripts/extract_batch_metrics.sh /var/log/batch/batch.log ./metrics/$(date +%Y%m%d)
   ```

3. **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤ëƒ…ìƒ·**
   ```bash
   # ë§¤ì¼ ë°°ì¹˜ ì‹¤í–‰ í›„
   psql -h localhost -U user -d techinsights -f scripts/export_batch_metadata.sql > ./metrics/$(date +%Y%m%d)/batch_metadata.csv
   ```

### Phase 3: ë¶„ì„ ë° ë¦¬í¬íŠ¸ (2-3ì¼)

1. **ë¡œê·¸ ë¶„ì„**
   ```bash
   python scripts/analyze_batch_logs.py /var/log/batch/batch.log > baseline_report.txt
   ```

2. **ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰**
   - ìœ„ì˜ SQL ì¿¼ë¦¬ë“¤ì„ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ì €ì¥

3. **Excel/CSV ì •ë¦¬**
   - ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¡œ ì •ë¦¬
   - ê·¸ë˜í”„ ìƒì„± (ì²˜ë¦¬ ì‹œê°„ ì¶”ì´, ì‹¤íŒ¨ìœ¨ ë“±)

---

## 5. ê¸°ëŒ€ ë©”íŠ¸ë¦­ ë° KPI

### 5.1 ìˆ˜ì§‘í•  í•µì‹¬ ë©”íŠ¸ë¦­

| ì¹´í…Œê³ ë¦¬ | ë©”íŠ¸ë¦­ | ëª©í‘œ ê°’ | ì¸¡ì • ë°©ë²• |
|---------|--------|---------|----------|
| **ì„±ëŠ¥** |
| | ì „ì²´ ë°°ì¹˜ ì†Œìš” ì‹œê°„ | < 30ë¶„ | Job Execution Duration |
| | íšŒì‚¬ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„ | < 2ë¶„ | Company Metrics í‰ê·  |
| | ê°€ì¥ ëŠë¦° íšŒì‚¬ ì²˜ë¦¬ ì‹œê°„ | < 5ë¶„ | Company Metrics ìµœëŒ“ê°’ |
| | ì²˜ë¦¬ëŸ‰ (throughput) | > 100 posts/min | Total Write / Duration |
| **ì•ˆì •ì„±** |
| | ë°°ì¹˜ ì„±ê³µë¥  | > 95% | Successful Jobs / Total Jobs |
| | íšŒì‚¬ë³„ ì„±ê³µë¥  | > 90% | Per-company Success Rate |
| | Skip ë¹„ìœ¨ | < 5% | Skip Count / Read Count |
| **ë³‘ë ¬ì„±** |
| | ë™ì‹œ ì²˜ë¦¬ íšŒì‚¬ ìˆ˜ | 1 (ìˆœì°¨) | í˜„ì¬ ì•„í‚¤í…ì²˜ ì œì•½ |
| | CPU ì‚¬ìš©ë¥  | < 30% | ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ |
| **ë°ì´í„° í’ˆì§ˆ** |
| | ì¤‘ë³µ ë°©ì§€ìœ¨ | 100% | ì¤‘ë³µ URL ê²€ì¶œ |
| | ë°ì´í„° ì‹ ì„ ë„ | < 24ì‹œê°„ | Last Crawl Time |

### 5.2 ë¶„ì„í•  ì§ˆë¬¸ë“¤

1. **ì™¸ë¶€ ì˜ì¡´ì„± ì§€ì—° (ì¡°ê±´ 1)**
   - Q: ê°€ì¥ ëŠë¦° íšŒì‚¬ëŠ” ì–´ë””ì´ë©°, ì–¼ë§ˆë‚˜ ëŠë¦°ê°€?
   - Q: íŠ¹ì • íšŒì‚¬ì˜ íƒ€ì„ì•„ì›ƒì´ ì „ì²´ ë°°ì¹˜ ì‹œê°„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?
   - ì¸¡ì •: `max(company_duration) / avg(company_duration)` ë¹„ìœ¨

2. **ì‹¤íŒ¨ ì¬ì‹¤í–‰ (ì¡°ê±´ 2)**
   - Q: ì§€ë‚œ 30ì¼ê°„ ì‹¤íŒ¨í•œ íšŒì‚¬ëŠ” ì´ ëª‡ ê°œì¸ê°€?
   - Q: ë™ì¼ íšŒì‚¬ê°€ ë°˜ë³µì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ”ê°€?
   - ì¸¡ì •: Failure í…Œì´ë¸” ì¿¼ë¦¬

3. **SLA (ì¡°ê±´ 3)**
   - Q: í˜„ì¬ í‰ê·  ë°°ì¹˜ ì™„ë£Œ ì‹œê°„ì€?
   - Q: ìµœì•…ì˜ ê²½ìš° ë°°ì¹˜ ì™„ë£Œ ì‹œê°„ì€?
   - Q: SLAë¥¼ 30ë¶„ìœ¼ë¡œ ì„¤ì •í–ˆì„ ë•Œ ì¤€ìˆ˜ìœ¨ì€?
   - ì¸¡ì •: Job Duration í†µê³„

4. **ëª¨ë‹ˆí„°ë§ (ì¡°ê±´ 4)**
   - Q: í˜„ì¬ ë°°ì¹˜ê°€ ì–´ëŠ íšŒì‚¬ë¥¼ ì²˜ë¦¬ ì¤‘ì¸ì§€ ì•Œ ìˆ˜ ìˆëŠ”ê°€?
   - Q: ì‹¤íŒ¨í•œ íšŒì‚¬ ëª©ë¡ì„ ì¦‰ì‹œ í™•ì¸í•  ìˆ˜ ìˆëŠ”ê°€?
   - ì¸¡ì •: ë¡œê·¸ ë¶„ì„

5. **ì¥ì•  ì¶”ì  (ì¡°ê±´ 5)**
   - Q: ì‹¤íŒ¨ ìœ í˜•ë³„ ë¶„í¬ëŠ”?
   - Q: ê°€ì¥ í”í•œ ì‹¤íŒ¨ ì›ì¸ì€?
   - ì¸¡ì •: Exception Type ë¶„ë¥˜

6. **ì•ˆì „í•œ ì¬ì‹¤í–‰ (ì¡°ê±´ 6)**
   - Q: Skip Limit 10ê°œëŠ” ì ì ˆí•œê°€?
   - Q: ì „ì²´ íšŒì‚¬ì˜ ëª‡ %ê°€ ì‹¤íŒ¨í•´ë„ ê´œì°®ì€ê°€?
   - ì¸¡ì •: Skip Count í†µê³„

7. **ë°ì´í„° ì‹ ì„ ë„ (ì¡°ê±´ 7)**
   - Q: ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ëŠ” ì–¸ì œ ìˆ˜ì§‘ë˜ì—ˆëŠ”ê°€?
   - Q: 24ì‹œê°„ ì´ë‚´ ë°ì´í„° ë¹„ìœ¨ì€?
   - ì¸¡ì •: Post.publishedAt ë¶„ì„

8. **Idempotency (ì¡°ê±´ 8)**
   - Q: ì¤‘ë³µ URLì´ ì €ì¥ë˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ê°€?
   - Q: ì¬ì‹¤í–‰ ì‹œ Gemini APIë¥¼ ì¤‘ë³µ í˜¸ì¶œí•˜ëŠ”ê°€?
   - ì¸¡ì •: ë¡œê·¸ ë¶„ì„, API í˜¸ì¶œ íšŸìˆ˜

---

## 6. ë¦¬í¬íŠ¸ í…œí”Œë¦¿

### 6.1 ì£¼ê°„ Baseline ë¦¬í¬íŠ¸

```markdown
# Batch Baseline Report - Week of YYYY-MM-DD

## Executive Summary
- Total Batch Runs: XX
- Success Rate: XX%
- Average Duration: XXm XXs
- Total Items Processed: XXXX

## Performance Metrics

### Job-Level Performance
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Avg Duration | XXm | 30m | ğŸ”´/ğŸŸ¡/ğŸŸ¢ |
| Max Duration | XXm | 45m | ğŸ”´/ğŸŸ¡/ğŸŸ¢ |
| Throughput | XX items/sec | 10 items/sec | ğŸ”´/ğŸŸ¡/ğŸŸ¢ |

### Company-Level Performance
| Company | Avg Time | Max Time | Success Rate | Posts/Run |
|---------|----------|----------|--------------|-----------|
| Company A | Xs | Xs | XX% | XX |
| ... |

## Failure Analysis

### Failure Rate by Type
| Type | Count | Percentage |
|------|-------|------------|
| Timeout | XX | XX% |
| Connection | XX | XX% |
| Parse Error | XX | XX% |

### Top Failing Companies
1. Company A - XX failures (Reason: ...)
2. Company B - XX failures (Reason: ...)

## Recommendations

### High Priority
1. [Issue]: íŠ¹ì • íšŒì‚¬ íƒ€ì„ì•„ì›ƒ ë¹ˆë²ˆ
   - Impact: ì „ì²´ ë°°ì¹˜ ì‹œê°„ XX% ì¦ê°€
   - Recommendation: ë³‘ë ¬ ì²˜ë¦¬ ë„ì…

### Medium Priority
...

## Appendix
- Raw Data: [ë§í¬]
- SQL Queries: [ë§í¬]
- Log Files: [ë§í¬]
```

---

## 7. ë°ì´í„° ì‹œê°í™”

### 7.1 Grafana ëŒ€ì‹œë³´ë“œ (ì„ íƒì‚¬í•­)

Spring Boot Actuator + Micrometerë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ì„ Prometheusë¡œ exportí•˜ê³  Grafanaì—ì„œ ì‹œê°í™”

**ì¶”ê°€ dependency:**
```kotlin
// build.gradle.kts
dependencies {
    implementation("org.springframework.boot:spring-boot-starter-actuator")
    implementation("io.micrometer:micrometer-registry-prometheus")
}
```

**application.yml:**
```yaml
management:
  endpoints:
    web:
      exposure:
        include: health,metrics,prometheus
  metrics:
    export:
      prometheus:
        enabled: true
```

### 7.2 ê°„ë‹¨í•œ HTML ë¦¬í¬íŠ¸ ìƒì„±

**íŒŒì¼ ìœ„ì¹˜:** `scripts/generate_html_report.py`

```python
#!/usr/bin/env python3
"""
ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ì„ HTML ë¦¬í¬íŠ¸ë¡œ ìƒì„±
"""

import json
from datetime import datetime

def generate_html_report(metrics_data, output_file='baseline_report.html'):
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Batch Baseline Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
            th {{ background-color: #4CAF50; color: white; }}
            .metric-good {{ color: green; font-weight: bold; }}
            .metric-warning {{ color: orange; font-weight: bold; }}
            .metric-bad {{ color: red; font-weight: bold; }}
            h1 {{ color: #333; }}
            h2 {{ color: #666; border-bottom: 2px solid #4CAF50; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š Batch System Baseline Report</h1>
        <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>

        <h2>Job Performance Summary</h2>
        <table>
            <tr>
                <th>Metric</th>
                <th>Current Value</th>
                <th>Target</th>
                <th>Status</th>
            </tr>
            <!-- ë°ì´í„°ë¥¼ ì±„ì›Œë„£ê¸° -->
        </table>

        <h2>Company Processing Times</h2>
        <table>
            <tr>
                <th>Company</th>
                <th>Avg Duration</th>
                <th>Success Rate</th>
            </tr>
            <!-- ë°ì´í„°ë¥¼ ì±„ì›Œë„£ê¸° -->
        </table>
    </body>
    </html>
    """

    with open(output_file, 'w') as f:
        f.write(html)

    print(f"HTML report generated: {output_file}")

if __name__ == '__main__':
    # ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ HTML ìƒì„±
    generate_html_report({})
```

---

## 8. ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°ì´í„° ìˆ˜ì§‘ ì¤€ë¹„
- [ ] BaselineMetricsListener ì½”ë“œ ì¶”ê°€
- [ ] ë¡œê·¸ ë ˆë²¨ ë° íŒŒì¼ ì„¤ì •
- [ ] ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ (`chmod +x scripts/*.sh`)
- [ ] ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±

### ìˆ˜ì§‘ ê¸°ê°„ (7-14ì¼)
- [ ] ë§¤ì¼ ë°°ì¹˜ ì‹¤í–‰
- [ ] ë§¤ì¼ ë¡œê·¸ ë°±ì—…
- [ ] ë§¤ì¼ ë©”íŠ¸ë¦­ ì¶”ì¶œ
- [ ] ì£¼ê°„ ì¤‘ê°„ ì ê²€

### ë¶„ì„
- [ ] SQL ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥
- [ ] ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- [ ] ë°ì´í„° ì‹œê°í™” (ê·¸ë˜í”„)
- [ ] Baseline ë¦¬í¬íŠ¸ ì‘ì„±

### ë¬¸ì„œí™”
- [ ] ì¸¡ì • ë°©ë²• ë¬¸ì„œí™”
- [ ] ì›ì‹œ ë°ì´í„° ë³´ê´€
- [ ] ê°œì„  ì „/í›„ ë¹„êµë¥¼ ìœ„í•œ í˜•ì‹ í†µì¼

---

ì´ì œ ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë©´ ê°œì„  ì‘ì—…ì˜ ëª…í™•í•œ ê·¼ê±°ë¥¼ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
